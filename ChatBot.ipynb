{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134a25a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\r\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8011d33",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3cf4b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d98c3",
   "metadata": {},
   "source": [
    "#### Read In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5fc2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines_features = [\"LineID\", \"Character\", \"Movie\", \"Name\", \"Line\"]\n",
    "movie_lines = pd.read_csv(\"cornell movie-dialogs corpus/movie_lines.txt\", \n",
    "                          sep = \"\\+\\+\\+\\$\\+\\+\\+\", \n",
    "                          engine = \"python\", \n",
    "                          index_col = False, names = movie_lines_features)\n",
    "\n",
    "# Using only the required columns, namely, \"LineID\" and \"Line\"\n",
    "movie_lines = movie_lines[[\"LineID\", \"Line\"]]\n",
    "\n",
    "# Strip the space from \"LineID\" for further usage and change the datatype of \"Line\"\n",
    "movie_lines[\"LineID\"] = movie_lines[\"LineID\"].apply(str.strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f9b4de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304713, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineID</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LineID           Line\n",
       "0  L1045   They do not!\n",
       "1  L1044    They do to!\n",
       "2   L985     I hope so.\n",
       "3   L984      She okay?\n",
       "4   L925      Let's go."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(movie_lines.shape)\n",
    "movie_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1fd59b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_conversations_features = [\"Character1\", \"Character2\", \"Movie\", \"Conversation\"]\n",
    "movie_conversations = pd.read_csv(\"cornell movie-dialogs corpus/movie_conversations.txt\", \n",
    "                                  sep = \"\\+\\+\\+\\$\\+\\+\\+\", \n",
    "                                  engine = \"python\", \n",
    "                                  index_col = False, \n",
    "                                  names = movie_conversations_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7736ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83097, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character1</th>\n",
       "      <th>Character2</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L194', 'L195', 'L196', 'L197']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L198', 'L199']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L200', 'L201', 'L202', 'L203']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L204', 'L205', 'L206']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L207', 'L208']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character1 Character2 Movie                       Conversation\n",
       "0        u0         u2    m0    ['L194', 'L195', 'L196', 'L197']\n",
       "1        u0         u2    m0                    ['L198', 'L199']\n",
       "2        u0         u2    m0    ['L200', 'L201', 'L202', 'L203']\n",
       "3        u0         u2    m0            ['L204', 'L205', 'L206']\n",
       "4        u0         u2    m0                    ['L207', 'L208']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(movie_conversations.shape)\n",
    "movie_conversations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7139e83",
   "metadata": {},
   "source": [
    "#### Create utterance and response pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a7ed4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unneeded chars from Converstation column\n",
    "def remove_chars(conv_str):\n",
    "    # input: \" ['L194', 'L195', 'L196', 'L197']\"\n",
    "    # output: 'L194,L195,L196,L197'\n",
    "    chars_to_remove = [' ', \"'\", '[', ']']\n",
    "    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "    return re.sub(rx, \"\",  conv_str)\n",
    "\n",
    "\n",
    "# generate pairs of consecutive lines out of conversations\n",
    "def generate_pairs(conv_list):\n",
    "    # input: ['L194', 'L195', 'L196', 'L197'])\n",
    "    # output: [['L194', 'L195'], ['L195', 'L196'], ['L196', 'L197']]\n",
    "    \n",
    "    return [[conv_list[i], conv_list[i+1]] for i in range(len(conv_list)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0bb0f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pairs of lines\n",
    "movie_conversations['conv_pairs'] = movie_conversations[\"Conversation\"].apply(lambda x: generate_pairs(remove_chars(x).split(',')))\n",
    "\n",
    "# expand conv_pairs column\n",
    "pairs = movie_conversations['conv_pairs'].apply(pd.Series, 1).stack()\n",
    "pairs.index = pairs.index.droplevel(-1)\n",
    "pair_df = pairs.apply(lambda x: pd.Series(x))\n",
    "pair_df.columns = ['L1','L2']\n",
    "\n",
    "movie_conversations = movie_conversations.join(pair_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea183ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221616, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character1</th>\n",
       "      <th>Character2</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Conversation</th>\n",
       "      <th>conv_pairs</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L194', 'L195', 'L196', 'L197']</td>\n",
       "      <td>[[L194, L195], [L195, L196], [L196, L197]]</td>\n",
       "      <td>L194</td>\n",
       "      <td>L195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L194', 'L195', 'L196', 'L197']</td>\n",
       "      <td>[[L194, L195], [L195, L196], [L196, L197]]</td>\n",
       "      <td>L195</td>\n",
       "      <td>L196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L194', 'L195', 'L196', 'L197']</td>\n",
       "      <td>[[L194, L195], [L195, L196], [L196, L197]]</td>\n",
       "      <td>L196</td>\n",
       "      <td>L197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L198', 'L199']</td>\n",
       "      <td>[[L198, L199]]</td>\n",
       "      <td>L198</td>\n",
       "      <td>L199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L200', 'L201', 'L202', 'L203']</td>\n",
       "      <td>[[L200, L201], [L201, L202], [L202, L203]]</td>\n",
       "      <td>L200</td>\n",
       "      <td>L201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character1 Character2 Movie                       Conversation  \\\n",
       "0        u0         u2    m0    ['L194', 'L195', 'L196', 'L197']   \n",
       "0        u0         u2    m0    ['L194', 'L195', 'L196', 'L197']   \n",
       "0        u0         u2    m0    ['L194', 'L195', 'L196', 'L197']   \n",
       "1        u0         u2    m0                    ['L198', 'L199']   \n",
       "2        u0         u2    m0    ['L200', 'L201', 'L202', 'L203']   \n",
       "\n",
       "                                   conv_pairs    L1    L2  \n",
       "0  [[L194, L195], [L195, L196], [L196, L197]]  L194  L195  \n",
       "0  [[L194, L195], [L195, L196], [L196, L197]]  L195  L196  \n",
       "0  [[L194, L195], [L195, L196], [L196, L197]]  L196  L197  \n",
       "1                              [[L198, L199]]  L198  L199  \n",
       "2  [[L200, L201], [L201, L202], [L202, L203]]  L200  L201  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(movie_conversations.shape)\n",
    "movie_conversations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e89f4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221616, 9)\n"
     ]
    }
   ],
   "source": [
    "conv_df = movie_conversations.merge(movie_lines, left_on='L1', right_on='LineID') \\\n",
    "                   .drop('LineID', axis=1) \\\n",
    "                   .rename(columns={'Line':'line_1'}) \\\n",
    "                   .merge(movie_lines, left_on='L2', right_on='LineID') \\\n",
    "                   .drop('LineID', axis=1) \\\n",
    "                   .rename(columns={'Line':'line_2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fd627ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221616, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-131-c65d9d6958fb>:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character1</th>\n",
       "      <th>Character2</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Conversation</th>\n",
       "      <th>conv_pairs</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>line_1</th>\n",
       "      <th>line_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33167</th>\n",
       "      <td>u1357</td>\n",
       "      <td>u1364</td>\n",
       "      <td>m90</td>\n",
       "      <td>['L284619', 'L284620', 'L284621', 'L284622', 'L284623', 'L284624', 'L284625', 'L284626', 'L284627', 'L284628', 'L284629', 'L284630', 'L284631', 'L284632', 'L284633']</td>\n",
       "      <td>[[L284619, L284620], [L284620, L284621], [L284621, L284622], [L284622, L284623], [L284623, L284624], [L284624, L284625], [L284625, L284626], [L284626, L284627], [L284627, L284628], [L284628, L284629], [L284629, L284630], [L284630, L284631], [L284631, L284632], [L284632, L284633]]</td>\n",
       "      <td>L284630</td>\n",
       "      <td>L284631</td>\n",
       "      <td>Hello, Max, Hildy Johnson. Was there an old lady --?</td>\n",
       "      <td>Butch! I'd put my arm in fire for you -- up to here!  Now, you can't double-cross me!... She does? All right -- put her on. I'll talk to her... Hello! Oh, hello, Madam... Now listen, you ten-cent glamour girl, you can't keep Butch away from his duty... What's that? You say that again and I'll come over there and knock your eye out! Hello?  I'll kill 'em! I'll kill both of 'em!  Duffy!  Mousing around with some big blonde Annie on my time! That's co-operation!  Duffy!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Character1 Character2  Movie  \\\n",
       "33167  u1357       u1364      m90    \n",
       "\n",
       "                                                                                                                                                                 Conversation  \\\n",
       "33167   ['L284619', 'L284620', 'L284621', 'L284622', 'L284623', 'L284624', 'L284625', 'L284626', 'L284627', 'L284628', 'L284629', 'L284630', 'L284631', 'L284632', 'L284633']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                     conv_pairs  \\\n",
       "33167  [[L284619, L284620], [L284620, L284621], [L284621, L284622], [L284622, L284623], [L284623, L284624], [L284624, L284625], [L284625, L284626], [L284626, L284627], [L284627, L284628], [L284628, L284629], [L284629, L284630], [L284630, L284631], [L284631, L284632], [L284632, L284633]]   \n",
       "\n",
       "            L1       L2  \\\n",
       "33167  L284630  L284631   \n",
       "\n",
       "                                                      line_1  \\\n",
       "33167   Hello, Max, Hildy Johnson. Was there an old lady --?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          line_2  \n",
       "33167   Butch! I'd put my arm in fire for you -- up to here!  Now, you can't double-cross me!... She does? All right -- put her on. I'll talk to her... Hello! Oh, hello, Madam... Now listen, you ten-cent glamour girl, you can't keep Butch away from his duty... What's that? You say that again and I'll come over there and knock your eye out! Hello?  I'll kill 'em! I'll kill both of 'em!  Duffy!  Mousing around with some big blonde Annie on my time! That's co-operation!  Duffy!!  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(conv_df.shape)\n",
    "\n",
    "# take a look at data\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "conv_df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787bd226",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1442f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Remove all english stopwords\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in (stopwords)])\n",
    "    return text\n",
    "\n",
    "def clean_text(string):\n",
    "    string = string.strip().lower()\n",
    "    # remove remove unwanted characters (restrict chars to a closed set)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`:]\", \" \", string)\n",
    "    # separate “‘s”, \"'ve\", r\"n't\", \"'re\", \"'d\", '\"'ll\" from words to help the tokenizer.\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    # Punctuation will be separated from text to help the tokenizer.\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    # Normalize spaces to a single space.\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    # If the token is a number, we substitute it with the “NUM” token\n",
    "    string = re.sub(r\"[0-9]+\", \"NUM\", string)\n",
    "    \n",
    "    # remove stopwords\n",
    "    # this would remove some esential words like 'I', etc. which is necessary to generate response\n",
    "    # string = remove_stopwords(string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "2a0a7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_token(string):\n",
    "    # add start and end to indicate the start and end of each sentence.\n",
    "    string = \"start \" + string + \" end\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "af1dfdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well I assure you, Sir, I have no desire to create difficulties. 45'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Well I assure you, Sir, I have no desire to create difficulties. 45\"\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e613b290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well i assure you , sir , i have no desire to create difficulties NUM'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3984dd",
   "metadata": {},
   "source": [
    "#### Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "d99bedf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221616, 221616)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = conv_df['line_1'].astype(str).apply(lambda x: clean_text(x))\n",
    "y = conv_df['line_2'].astype(str).apply(lambda x: clean_text(x)).apply(lambda x: add_start_end_token(x))\n",
    "\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "\n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "1563a263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['can we make this quick? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again ',\n",
       "       \"well , i thought we 'd start with pronunciation , if that 's okay with you \",\n",
       "       'not the hacking and gagging and spitting part please ',\n",
       "       \"you 're asking me out that 's so cute what 's your name again?\",\n",
       "       \"no , no , it 's my fault we did n't have a proper introduction \"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e3008d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"start well , i thought we 'd start with pronunciation , if that 's okay with you  end\",\n",
       "       'start not the hacking and gagging and spitting part please  end',\n",
       "       \"start okay then how 'bout we try out some french cuisine saturday? night? end\",\n",
       "       'start forget it  end', 'start cameron  end'], dtype=object)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "9dd73cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221616"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "28c70610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "\n",
    "def format_input(x, y, MAX_NB_WORDS=100000, VALIDATION_SPLIT=0.3, MAX_SEQUENCE_LENGTH=1000, sample_ratio=1):\n",
    "    # format our text samples and labels into tensors that can be fed into a neural network\n",
    "    # Input:\n",
    "    #     x: numpy array of utterances\n",
    "    #     y: numpy array of responses\n",
    "    # Output:\n",
    "    #     x_train, y_train, x_val, y_val\n",
    "    sample_size = int(len(x) * sample_ratio)\n",
    "    x = x[:sample_size]\n",
    "    y = y[:sample_size]\n",
    "    texts = np.concatenate((x,y))\n",
    "    \n",
    "    # initialize a tokenizer\n",
    "    tokenizer = Tokenizer(nb_words=MAX_NB_WORDS,\n",
    "                          oov_token='<OOV>')\n",
    "\n",
    "    # Updates the internal word-index dictionary (tokenizer.word_index) in the order of word frequency (lower integer means more frequent)\n",
    "    # 0 is reserved for padding. \n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    print('word_index is like', dict(list(word_index.items())[:5]))\n",
    "\n",
    "    \n",
    "    # Transforms each texts into a sequence of integers \n",
    "    # by replacing each token into the index in tokenizer.word_index\n",
    "    x_vec = tokenizer.texts_to_sequences(x)\n",
    "    y_vec = tokenizer.texts_to_sequences(y)\n",
    "    \n",
    "    sample_ind = random.randint(0, len(x))\n",
    "\n",
    "    print('len(x_vec), len(y_vec): ', len(x_vec), len(y_vec))\n",
    "    print(f'len(x[{sample_ind}]), len(x_vec[{sample_ind}]): ', len(x[sample_ind].split(' ')), len(x_vec[sample_ind]))\n",
    "    print(f'x[{sample_ind}] is like: \\n\\t', x[sample_ind])\n",
    "    print(f'x_vec[{sample_ind}] is like: \\n\\t', x_vec[sample_ind])\n",
    "    print(f'y[{sample_ind}] is like: \\n\\t', y[sample_ind])\n",
    "    print(f'y_vec[{sample_ind}] is like: \\n\\t', y_vec[sample_ind])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pad as deep learning methods using sequential data require fixed length\n",
    "    x_padded = pad_sequences(x_vec, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    y_padded = pad_sequences(y_vec, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "    print('len(x_padded), len(y_padded): ', len(x_padded), len(y_padded))\n",
    "    print(f'len(x_padded[{sample_ind}]): ', len(x_padded[sample_ind]))\n",
    "    print(f'x_padded[{sample_ind}] is like: \\n\\t',  x_padded[sample_ind])\n",
    "    print(f'y_padded[{sample_ind}] is like: \\n\\t',  y_padded[sample_ind])\n",
    "\n",
    "\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * x.shape[0])\n",
    "\n",
    "    x_train = x_padded[:-nb_validation_samples]\n",
    "    y_train = y_padded[:-nb_validation_samples]\n",
    "    x_val = x_padded[-nb_validation_samples:]\n",
    "    y_val = y_padded[-nb_validation_samples:]\n",
    "\n",
    "    print('len(x_train), len(y_train), len(x_val), len(y_val): ', len(x_train), len(y_train), len(x_val), len(y_val))\n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    x_val = np.array(x_val)\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    return tokenizer, x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "10721f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35274 unique tokens.\n",
      "word_index is like {'<OOV>': 1, 'start': 2, 'end': 3, 'you': 4, 'i': 5}\n",
      "len(x_vec), len(y_vec):  110808 110808\n",
      "len(x[40538]), len(x_vec[40538]):  7 7\n",
      "x[40538] is like: \n",
      "\t where 's sarah? where are the boys?\n",
      "x_vec[40538] is like: \n",
      "\t [84, 9, 2037, 84, 36, 6, 534]\n",
      "y[40538] is like: \n",
      "\t start sit down , jake  end\n",
      "y_vec[40538] is like: \n",
      "\t [2, 404, 118, 848, 3]\n",
      "len(x_padded), len(y_padded):  110808 110808\n",
      "len(x_padded[40538]):  32\n",
      "x_padded[40538] is like: \n",
      "\t [  84    9 2037   84   36    6  534    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "y_padded[40538] is like: \n",
      "\t [  2 404 118 848   3   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "len(x_train), len(y_train), len(x_val), len(y_val):  77566 77566 33242 33242\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "# MAX_SEQUENCE_LENGTH = max([len(text) for text in texts])\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "tokenizer, x_train, y_train, x_val, y_val = format_input(x, y, MAX_NB_WORDS=MAX_NB_WORDS, VALIDATION_SPLIT=0.3, \n",
    "                                                         MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH,\n",
    "                                                        sample_ratio=0.5)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word\n",
    "vocab_size = len(word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85d3cf8",
   "metadata": {},
   "source": [
    "## Transfer Embedding from pre-trained GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "baadac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "9ed59a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix....\n",
      "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state-of-the-art semantic...\n",
      "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipedia 2017, UMBC webba...\n",
      "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vo...\n",
      "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M voc...\n",
      "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M voc...\n",
      "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M voc...\n",
      "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B ...\n",
      "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B...\n",
      "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B...\n",
      "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B...\n",
      "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of the Google News dat...\n",
      "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors trained on full Russian...\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import json\n",
    "\n",
    "# check available pre-trained models\n",
    "info = api.info()\n",
    "\n",
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(\n",
    "        '%s (%d records): %s' % (\n",
    "            model_name,\n",
    "            model_data.get('num_records', -1),\n",
    "            model_data['description'][:60] + '...',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e9f250b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"num_records\": 400000,\n",
      "    \"file_size\": 134300434,\n",
      "    \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
      "    \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py\",\n",
      "    \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "    \"parameters\": {\n",
      "        \"dimension\": 100\n",
      "    },\n",
      "    \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "    \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.\",\n",
      "    \"read_more\": [\n",
      "        \"https://nlp.stanford.edu/projects/glove/\",\n",
      "        \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "    ],\n",
      "    \"checksum\": \"40ec481866001177b8cd4cb0df92924f\",\n",
      "    \"file_name\": \"glove-wiki-gigaword-100.gz\",\n",
      "    \"parts\": 1\n",
      "}\n",
      "[=========-----------------------------------------] 18.3% 23.5/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================-----------------------------] 42.6% 54.6/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================----------------] 68.3% 87.4/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# get detailed info of a pre-trained model\n",
    "pret_model_name = 'glove-wiki-gigaword-100'\n",
    "detailed_info = api.info(pret_model_name)\n",
    "print(json.dumps(detailed_info, indent=4))\n",
    "\n",
    "glove_model = api.load(pret_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "82cc3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(word_index, embeddings_index, EMBEDDING_DIM):\n",
    "    # form embedding metrix for all words in vocabulary\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if word in embeddings_index:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "    return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "e095a869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35274, (35275, 100))"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = glove_model\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = get_embedding_matrix(word_index, embeddings_index, EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "len(word_index), embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56035e46",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "0d0512a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the attention layer from https://github.com/thushv89/attention_keras/blob/master/src/layers/attention.py\n",
    "# and copy it in a different file called attention.py. \n",
    "# This attention is an implementation of ‘Bahdanau Attention’ .\n",
    "from attention import AttentionLayer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "c8942146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 32, 100)      3527500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 32, 100), (N 80400       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 32, 100), (N 80400       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    3527500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 32, 100), (N 80400       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 100),  80400       embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 100),  20100       lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 200)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 35275)  7090275     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 14,486,975\n",
      "Trainable params: 7,431,975\n",
      "Non-trainable params: 7,055,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session() \n",
    "latent_dim = 100\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(MAX_SEQUENCE_LENGTH,)) \n",
    "# enc_emb_layer = Embedding(vocab_size, latent_dim, trainable=True)\n",
    "# use pre-trained word embeddings\n",
    "enc_emb_layer = Embedding(vocab_size,\n",
    "                            latent_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "enc_emb = enc_emb_layer(encoder_inputs)\n",
    "#LSTM 1 \n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "#LSTM 2 \n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "#LSTM 3 \n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "# dec_emb_layer = Embedding(vocab_size, latent_dim, trainable=True)\n",
    "# use pre-trained word embeddings\n",
    "dec_emb_layer = Embedding(vocab_size,\n",
    "                            latent_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "#Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "#Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab8c667",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1785/2424 [=====================>........] - ETA: 15:03 - loss: 2.4136 - accuracy: 0.6757"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "history = model.fit([x_train, y_train[:,:-1]], y_train.reshape(y_train.shape[0], y_train.shape[1],1)[:,1:], \n",
    "                    epochs=epochs, \n",
    "                    callbacks=[es],\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data = ([x_val, y_val[:,:-1]], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:,1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "60ee21d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_json = model.to_json()\n",
    "with open(\"chatbot_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"chatbot_model_weight.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "a5614b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model architecture and asigning the weights\n",
    "json_file = open('chatbot_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_loaded = model_from_json(loaded_model_json, custom_objects={'AttentionLayer': AttentionLayer})\n",
    "# load weights into new model\n",
    "model_loaded.load_weights(\"chatbot_model_weight.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed3b664",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "45028a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=500\n",
    "# encoder inference\n",
    "encoder_inputs = model_loaded.input[0]  #loading encoder_inputs\n",
    "encoder_outputs, state_h, state_c = model_loaded.layers[6].output #loading encoder_outputs\n",
    "#print(encoder_outputs.shape)\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "# decoder inference\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(32,latent_dim))\n",
    "# Get the embeddings of the decoder sequence\n",
    "decoder_inputs = model_loaded.layers[3].output\n",
    "#print(decoder_inputs.shape)\n",
    "dec_emb_layer = model_loaded.layers[5]\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_lstm = model_loaded.layers[7]\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "#attention inference\n",
    "attn_layer = model_loaded.layers[8]\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "concate = model_loaded.layers[9]\n",
    "decoder_inf_concat = concate([decoder_outputs2, attn_out_inf])\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_dense = model_loaded.layers[10]\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "                      [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "1893abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(word_index, index_word, input_seq, MAX_SEQUENCE_LENGTH):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # skip pad token\n",
    "        if sampled_token_index == 0:\n",
    "            break\n",
    "        else:\n",
    "            sampled_token = index_word[sampled_token_index]\n",
    "        if sampled_token != 'end':\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if sampled_token == 'end' or len(decoded_sentence.split()) >= (MAX_SEQUENCE_LENGTH - 1):\n",
    "            stop_condition = True\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "    return decoded_sentence\n",
    "\n",
    "def seq2summary(word_index, index_word, input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i != 0 and i != word_index['start']) and i != word_index['end']):\n",
    "            newString = newString + index_word[i] + ' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(index_word, input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        # 0 represents pading token\n",
    "        if(i != 0):\n",
    "            newString = newString + index_word[i] + ' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "78c56786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line_1: partnership \n",
      "Original Line_2: well you got to admit we come a long way \n",
      "Predicted Line_2:  i do n't know\n",
      "\n",
      "\n",
      "Line_1: you okay \n",
      "Original Line_2: yeah but i was n't there for a second \n",
      "Predicted Line_2:  i do n't know\n",
      "\n",
      "\n",
      "Line_1: yeah but i was n't there for a second \n",
      "Original Line_2: you did pick a real strange time to go and be brave all on your own \n",
      "Predicted Line_2:  i do n't know\n",
      "\n",
      "\n",
      "Line_1: okay reggie start bustin' my chops tell me how great you were with that chick \n",
      "Original Line_2: hey jack real men do n't have to go in for that macho bullshit but i was fantastic \n",
      "Predicted Line_2:  i do n't know\n",
      "\n",
      "\n",
      "Line_1: a minute cates i 've been waitin' three years for that i do n't think it 's fair man what about the merit system you were gonnna give me a few thousand \n",
      "Original Line_2: there 's nothin' to talk about \n",
      "Predicted Line_2:  i do n't know\n",
      "\n",
      "\n",
      "Line_1: it 's your money it 'll be here in six months when you get out \n",
      "Original Line_2: and you 're tellin' me you do n't want any of this cash \n",
      "Predicted Line_2:  i do n't know\n",
      "\n",
      "\n",
      "Line_1: and you 're tellin' me you do n't want any of this cash \n",
      "Original Line_2: that 's right not my style reggie \n",
      "Predicted Line_2:  i do n't know\n",
      "\n",
      "\n",
      "Line_1: that 's right not my style reggie \n",
      "Original Line_2: you are an awesomely weird cop sure wish there were more like you runnin' around out here \n",
      "Predicted Line_2:  i do n't know\n",
      "\n",
      "\n",
      "Line_1: you are an awesomely weird cop sure wish there were more like you runnin' around out here \n",
      "Original Line_2: no you do n't if i ever get word of you steppin' over the line again i'm gonna ventilate that suit of yours \n",
      "Predicted Line_2:  i do n't know\n",
      "\n",
      "\n",
      "Line_1: no you do n't if i ever get word of you steppin' over the line again i'm gonna ventilate that suit of yours \n",
      "Original Line_2: spare met jack i'm into legit investments from here on in \n",
      "Predicted Line_2:  i do n't know\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index_word = tokenizer.index_word\n",
    "for i in range(10):  \n",
    "    print(\"Line_1:\", seq2text(index_word, x_val[i]))\n",
    "    print(\"Original Line_2:\", seq2summary(word_index, index_word, y_val[i]))\n",
    "    print(\"Predicted Line_2:\", decode_sequence(word_index, index_word, x_val[i].reshape(1, 32), MAX_SEQUENCE_LENGTH))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ee1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO\n",
    "# clean code\n",
    "# run in colab to use gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef6a7e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
